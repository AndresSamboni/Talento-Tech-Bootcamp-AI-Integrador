{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmX1OtlwYCgC9YRgCLeL1q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <FONT COLOR=\"red\">***MARKOV DECISION PROCESS (MDP)***</FONT>\n","---\n","---\n","\n","Mathematic model rely on the **R**einforcement **L**earning (**RL**). **M**arkov **D**ecision **Process** (**MDP**) is a mathematic model widely in **RL**. Essentialy, **MDP** describe an environment in which an agent takes sequential decisions to maximize a cumulative reward over time."],"metadata":{"id":"W9VAGcTvQvGw"}},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**EXERCISE 1**</FONT>\n","---\n","---\n","\n","Defines a basic MDP with three states (A, B, C), two possible actions in each state (Up, Down), a random transition function and\n","random rewards associated with each action in each state."],"metadata":{"id":"hqyIfO_SRyRF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"eQZt8kjwQlhm","executionInfo":{"status":"ok","timestamp":1730507482002,"user_tz":300,"elapsed":322,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"outputs":[],"source":["# IMPORT COMMON LIBRARIES\n","import numpy as np"]},{"cell_type":"code","source":["# STATE, ACTIONS, AND REWARD DEFINITION\n","states = ['A','B','C']\n","actions = ['Up','Down']\n","rewards = np.random.randint(0,10,size=(len(states),len(actions)))"],"metadata":{"id":"VLXqJTVgSJgT","executionInfo":{"status":"ok","timestamp":1730507768813,"user_tz":300,"elapsed":211,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# RANDOM TRANSITION FUNCTION\n","def random_transition ():\n","  return np.random.choice(states)"],"metadata":{"id":"UVe2ARKtSi4W","executionInfo":{"status":"ok","timestamp":1730507618938,"user_tz":300,"elapsed":235,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# TEST FUNCTION\n","actual_state = np.random.choice(states)\n","action = np.random.choice(actions)\n","new_state = random_transition()\n","reward = rewards[states.index(actual_state),actions.index(action)]\n","\n","# DISPLAY DATA\n","print(f'Actual state: {actual_state}')\n","print(f'Action: {action}')\n","print(f'New state: {new_state}')\n","print(f'Reward: {reward}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26vneCNqS_4I","executionInfo":{"status":"ok","timestamp":1730507940551,"user_tz":300,"elapsed":239,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"1a4e265a-ee4f-4125-fe5e-7e8c0d140c0e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Actual state: A\n","Action: Up\n","New state: C\n","Reward: 9\n"]}]},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**EXERCISE 2**</FONT>\n","---\n","---\n","\n","Write a function to calculate the value function of a state given an MDP, using the value iteration algorithm."],"metadata":{"id":"iQe0j3mlTfj4"}},{"cell_type":"code","source":["# VALUES FUNCTION\n","def state_value (mdp, gamma=0.9, theta=0.01):\n","  # OBTAIN VALUES\n","  values = {state: 0 for state in mdp.states}\n","  while True:\n","    delta = 0\n","    for state in mdp.states:\n","      prev_value = values[state]\n","      values[state] = sum(\n","        mdp.transitions[state][action][new_state] * (\n","          mdp.reward[state][action][new_state] + gamma * values[new_state]\n","        ) for action in mdp.actions for new_state in mdp.states\n","      )\n","      delta = max(delta, abs(prev_value - values[state]))\n","      if delta < theta:\n","        break\n","      return values"],"metadata":{"id":"khS6tsLATrip","executionInfo":{"status":"ok","timestamp":1730508332498,"user_tz":300,"elapsed":227,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# MDP CLASS\n","class MDP:\n","  # CONSTRUCTOR CLASS\n","  def __init__(self, states, actions, transitions, rewards):\n","    self.states = states\n","    self.actions = actions\n","    self.transitions = transitions\n","    self.rewards = {}\n","\n","    for state in states:\n","      self.rewards[state] = {}\n","      for action in actions:\n","          self.rewards[state][action] = {}\n","          for next_state in states:\n","              # Assign a reward value for this state-action-next_state combination\n","              # Replace this with your actual reward logic\n","              self.rewards[state][action][next_state] = 0  # Default reward of 0"],"metadata":{"id":"6wveL_K2cYfH","executionInfo":{"status":"ok","timestamp":1730514455372,"user_tz":300,"elapsed":8,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# DEFINE STATES, ACTIONS, TRANSITIONS, AND REWARDS\n","states = ['A', 'B', 'C']\n","actions = ['Up', 'Down']\n","transitions = {\n","  'A': {'Up': {'A': 0.5, 'B': 0.5, 'C': 0.0}, 'Down': {'A': 0.0, 'B': 0.8, 'C': 0.2}},\n","  'B': {'Up': {'A': 0.2, 'B': 0.3, 'C': 0.5}, 'Down': {'A': 0.1, 'B': 0.6, 'C': 0.3}},\n","  'C': {'Up': {'A': 0.7, 'B': 0.1, 'C': 0.2}, 'Down': {'A': 0.3, 'B': 0.3, 'C': 0.4}}\n","}\n","rewards = {\n","  'A': {'Up': {'A': 1, 'B': 2, 'C': 3}, 'Down': {'A': 4, 'B': 5, 'C': 6}},\n","  'B': {'Up': {'A': 7, 'B': 8, 'C': 9}, 'Down': {'A': 10, 'B': 11, 'C': 12}},\n","  'C': {'Up': {'A': 13, 'B': 14, 'C': 15}, 'Down': {'A': 16, 'B': 17, 'C': 18}}\n","}\n","\n","# CREATE AN INSTANCE OF THE MDP CLASS\n","mdp = MDP(states, actions, transitions, rewards)"],"metadata":{"id":"o1oLroYkc8nJ","executionInfo":{"status":"ok","timestamp":1730514458546,"user_tz":300,"elapsed":3,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# TEST STATE VALUES\n","state_values = state_value(mdp)\n","print(f'Value of the states: {state_values}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eu8tmp5QrshQ","executionInfo":{"status":"ok","timestamp":1730514192481,"user_tz":300,"elapsed":268,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"923aa81d-6ab0-469e-a53c-49c31f5c2bdc"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Value of the states: {'A': 6.7, 'B': 0, 'C': 0}\n"]}]},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**EXERCISE 3**</FONT>\n","---\n","---\n","\n","Write a function to check if\n","a given MDP satisfies the property of\n","Markov."],"metadata":{"id":"biblm4vrd6Yt"}},{"cell_type":"code","source":["# VERIFY MDP\n","def verify_MDP_property (mdp):\n","  for state in mdp.states:\n","    for action in mdp.actions:\n","      sum_probability = sum(mdp.transitions[state][action].values())\n","      if not np.isclose(sum_probability,1):\n","        return False\n","      return True"],"metadata":{"id":"CXoddu26gwjk","executionInfo":{"status":"ok","timestamp":1730514223169,"user_tz":300,"elapsed":270,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# TEST IMPLEMENTATION\n","print(f'Accomplish the Markov property: {verify_MDP_property(mdp)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fUpCmWbOh1NV","executionInfo":{"status":"ok","timestamp":1730514225001,"user_tz":300,"elapsed":7,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"3426b834-d155-467c-8af1-25164473cb86"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Accomplish the Markov property: True\n"]}]},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**EXERCISE 4**</FONT>\n","---\n","---\n","\n","Write a function to calculate the average reward per action in a MDP."],"metadata":{"id":"4BxF9Mstiwht"}},{"cell_type":"code","source":["# REWARD\n","def average_reward (mdp):\n","  total_reward = 0\n","  total_actions = 0\n","  for state in mdp.states:\n","    for action in mdp.actions:\n","      for new_state in mdp.states:\n","        total_reward += mdp.rewards[state][action][new_state]\n","        total_actions += 1\n","        return total_reward / total_actions"],"metadata":{"id":"aROhsKdDjVv1","executionInfo":{"status":"ok","timestamp":1730514229554,"user_tz":300,"elapsed":265,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# TEST IMPLEMENTATION\n","print(f'Average reward per action {average_reward(mdp)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOnCdjlhmgeK","executionInfo":{"status":"ok","timestamp":1730514464115,"user_tz":300,"elapsed":5,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"6365eaf9-e136-4c18-b983-ca27e63d456d"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Average reward per action 0.0\n"]}]}]}