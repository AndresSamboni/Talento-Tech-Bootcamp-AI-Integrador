{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMKX0p4liwPdAx675Yb2DZ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <FONT COLOR=\"red\">***EXAMPLE - SIMPLE GRIDWORLD AGENT***</FONT>\n","---\n","---\n","In this demonstration, I learn how to implement the Q-Learning algorithm in an environment of a simple gridworld. Gridworld is a grid where an agent can move between cells, with the objective of reaching a target cell while avoiding obstacles. Through the implementation of Q-Learning, I understand how to teach the agent to make decisions based on the received rewards in each state and action."],"metadata":{"id":"V2MK3NmZGLbC"}},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**Exercise 1: Implementation of Q-Learning in a Simple Gridworld environment**</FONT>\n","---\n","---\n","The first step is always importing the libraries. When we work with Reinforcement Learning (RL) in Google Colab, we use the NumPy library to manipulate the data in the simplest version."],"metadata":{"id":"MO5GcraEQaB8"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"txq_aDsdF-1r","executionInfo":{"status":"ok","timestamp":1731765550022,"user_tz":300,"elapsed":230,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"outputs":[],"source":["# IMPORT COMMON LIBRARIES\n","import numpy as np"]},{"cell_type":"code","source":["# GRIDWORLD DEFINITION\n","gridworld = np.array([\n","  [-1,-1,-1,1],\n","  [-1,-1,-1,-1],\n","  [-1,-1,-1,-1],\n","  [-1,-1,-1,-1],\n","])"],"metadata":{"id":"dIo7xHVuRR67","executionInfo":{"status":"ok","timestamp":1731765603607,"user_tz":300,"elapsed":234,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# DEFINITON OF THE POSSIBLE ACTIONS\n","actions = [\n","  (0,-1), # Up\n","  (0,1), # Down\n","  (-1,0), # Left\n","  (1,0) # Right\n","]"],"metadata":{"id":"BvRwI4tyRhEM","executionInfo":{"status":"ok","timestamp":1731765724762,"user_tz":300,"elapsed":210,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Q-LEARNING IMPLEMENTATION\n","Q = {}\n","for row in range(gridworld.shape[0]):\n","  for col in range(gridworld.shape[1]):\n","    Q[(row, col)] = [0] * len(actions)"],"metadata":{"id":"i3N7UqOOR-cE","executionInfo":{"status":"ok","timestamp":1731766475736,"user_tz":300,"elapsed":205,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# TRAIN PARAMETERS\n","gamma = 0.8\n","alpha = 0.1\n","num_episodes = 1000"],"metadata":{"id":"66iIlT8vSF3J","executionInfo":{"status":"ok","timestamp":1731765779943,"user_tz":300,"elapsed":203,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# TRAIN LOOP\n","for _ in range(num_episodes):\n","  # Initial cell\n","  state = (0,0)\n","  while state != (0,3): # Target cell\n","    action = np.random.choice(range(len(actions)))\n","    new_row = state[0] + actions[action][0]\n","    new_col = state[1] + actions[action][1]\n","    if 0 <= new_row < gridworld.shape[0] and 0 <= new_col < gridworld.shape[1]:\n","      reward = gridworld[new_row,new_col]\n","      new_value = reward + gamma * np.max(Q[new_row,new_col])\n","      # Q-Larning Equation\n","      Q[state][action] = (1-alpha) * Q[state][action] + alpha * new_value\n","      # New State\n","      state = (new_row, new_col)"],"metadata":{"id":"MMIFP7ooSL-K","executionInfo":{"status":"ok","timestamp":1731766487115,"user_tz":300,"elapsed":2052,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Q-VALUES AFTER TRAINING\n","print('Q-Values after training:')\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1id9KCeDTccd","executionInfo":{"status":"ok","timestamp":1731766490606,"user_tz":300,"elapsed":264,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"5b8ffa8c-4b25-4276-c31c-4997976886e6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-Values after training:\n","{(0, 0): [0, -0.9999999999999994, 0, -0.9999999999999994], (0, 1): [-0.9999999999999994, -0.20000000000000062, 0, -1.7999999999999987], (0, 2): [-0.9999999999999994, 0.9999999999999994, 0, -1.1600000000000024], (0, 3): [0, 0, 0, 0], (1, 0): [0, -1.7999999999999987, -0.9999999999999994, -0.9999999999999994], (1, 1): [-0.9999999999999994, -1.1600000000000024, -0.9999999999999994, -1.7999999999999987], (1, 2): [-1.7999999999999987, -0.20000000000000062, -0.20000000000000062, -1.7999999999999987], (1, 3): [-1.1600000000000024, 0, 0.9999999999999994, -0.9999999999999994], (2, 0): [0, -1.7999999999999987, -0.9999999999999994, -0.9999999999999994], (2, 1): [-0.9999999999999994, -1.7999999999999987, -1.7999999999999987, -0.9999999999999994], (2, 2): [-1.7999999999999987, -0.9999999999999994, -1.1600000000000024, -0.9999999999999994], (2, 3): [-1.7999999999999987, 0, -0.20000000000000062, -0.9999999999999994], (3, 0): [0, -0.9999999999999994, -0.9999999999999994, 0], (3, 1): [-0.9999999999999994, -0.9999999999999994, -1.7999999999999987, 0], (3, 2): [-0.9999999999999994, -0.9999999999999994, -1.7999999999999987, 0], (3, 3): [-0.9999999999999994, 0, -0.9999999999999994, 0]}\n"]}]},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**Exercise 2: Reinforcement Learning Application in Games**</FONT>\n","---\n","---"],"metadata":{"id":"EJtp-_uTVCPE"}},{"cell_type":"code","source":["# IMPORT COMMON LIBRARIES\n","import numpy as np"],"metadata":{"id":"w31fS8uRVQCs","executionInfo":{"status":"ok","timestamp":1731766603067,"user_tz":300,"elapsed":220,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# REWARD DEFINITION\n","reward = {\n","  'win': 1,\n","  'lose': -1,\n","  'draw': 0\n","}"],"metadata":{"id":"5Z3-J5a5VU3f","executionInfo":{"status":"ok","timestamp":1731766658941,"user_tz":300,"elapsed":212,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# IMPLEMENTATION OF THE Q-LEARNING\n","Q = {}\n","gamma = 0.8"],"metadata":{"id":"SC_G9URgViqM","executionInfo":{"status":"ok","timestamp":1731766962290,"user_tz":300,"elapsed":190,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# DEFINITION OF THE Q-LEARNING ALGORITHM\n","def q_learning_game(actual_state, action, new_state, result):\n","    # Convert actions to numerical indices\n","    actions = ['paper', 'rock', 'scissors']  # Define all possible actions\n","    action_index = actions.index(action)  # Get the numerical index of the current action\n","\n","    if actual_state not in Q:\n","        Q[actual_state] = np.zeros(len(actions))\n","    if new_state not in Q:\n","        Q[new_state] = np.zeros(len(actions))\n","\n","    # Use action_index for indexing\n","    new_value = reward[result] + gamma * np.max(Q[new_state])\n","    Q[actual_state][action_index] = (1 - alpha) * Q[actual_state][action_index] + alpha * new_value"],"metadata":{"id":"TUIgBDy3VtHE","executionInfo":{"status":"ok","timestamp":1731767229511,"user_tz":300,"elapsed":225,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# States are represented as the opponent's previous move (e.g., 'rock', 'paper', 'scissors')\n","# Actions are represented as the agent's move (e.g., 'rock', 'paper', 'scissors')\n","# Result is either 'win', 'lose', or 'draw'\n","\n","# Example usage:\n","actual_state = 'rock'  # Opponent's previous move was 'rock'\n","action = 'paper'  # Agent chooses to play 'paper'\n","new_state = 'paper'  # Opponent's next move (unknown, could be anything)\n","result = 'win'  # Agent wins this round\n","\n","q_learning_game(actual_state, action, new_state, result)"],"metadata":{"id":"M6AgMYd2XDlM","executionInfo":{"status":"ok","timestamp":1731767231165,"user_tz":300,"elapsed":231,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**Reinforcement Learning Application in Robotics**</FONT>\n","---\n","---"],"metadata":{"id":"7HZenSwBYe-e"}},{"cell_type":"code","source":["# IMPORT COMMON LIBRARIES\n","import numpy as np"],"metadata":{"id":"tUZgGpOoYp1E","executionInfo":{"status":"ok","timestamp":1731767496831,"user_tz":300,"elapsed":239,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# DEFINITION OF THE NEVIGATION ENVIRONMENT\n","environment = np.array([\n","  [0,0,0,0,0],\n","  [0,-1,-1,-1,0],\n","  [0,0,-1,0,0],\n","  [0,-1,-1,-1,0],\n","  [0,0,0,0,0]\n","])"],"metadata":{"id":"d9bw435LYvCW","executionInfo":{"status":"ok","timestamp":1731767581457,"user_tz":300,"elapsed":225,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# DEFINITION OF THE POSSIBLE ACTIONS\n","actions = [\n","  (0,-1), # Up\n","  (0,1), # Down\n","  (-1,0), # Left\n","  (1,0) # Right\n","]"],"metadata":{"id":"f4Z-PTFmZD07","executionInfo":{"status":"ok","timestamp":1731767605314,"user_tz":300,"elapsed":204,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Q-LEARNING IMPLEMENTATION\n","Q = np.zeros((environment.shape[0], environment.shape[1], len(actions)))"],"metadata":{"id":"fLvWmHAqZJlV","executionInfo":{"status":"ok","timestamp":1731767739900,"user_tz":300,"elapsed":192,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# TRAIN PARAMETERS\n","gamma = 0.9 # Discount factor\n","alpha = 0.1 # Learning rate\n","num_episodes = 1000"],"metadata":{"id":"N1Zv8X8sZqe2","executionInfo":{"status":"ok","timestamp":1731767787225,"user_tz":300,"elapsed":398,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# TRAIN LOOP\n","for _ in range(num_episodes):\n","  # Initial State\n","  state = (0,0)\n","  while True:\n","    action = np.random.choice(range(len(actions)))\n","    new_row = state[0] + actions[action][0]\n","    new_col = state[1] + actions[action][1]\n","    if 0 <= new_row < environment.shape[0] and 0 <= new_col < environment.shape[1]:\n","      reward = environment[new_row,new_col]\n","      new_value = reward + gamma * np.max(Q[new_row,new_col])\n","      Q[state[0], state[1], action] = (1 - alpha) * Q[state[0], state[1], action] + alpha * new_value\n","      # New State\n","      state = (new_row, new_col)\n","      # Break Point Loop\n","      break"],"metadata":{"id":"yVUHoPBUZ14a","executionInfo":{"status":"ok","timestamp":1731768585728,"user_tz":300,"elapsed":194,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Q-VALUES AFTER TRAIN\n","print('Q-Values after training:')\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DAK5ZfglaqUv","executionInfo":{"status":"ok","timestamp":1731768589836,"user_tz":300,"elapsed":230,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"895ff20d-f60b-487a-d049-ff2ae04208d8"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-Values after training:\n","[[[ 0.  0.  0.  0.]\n","  [ 0.  0.  0. -1.]\n","  [ 0.  0.  0. -1.]\n","  [ 0.  0.  0. -1.]\n","  [ 0.  0.  0.  0.]]\n","\n"," [[ 0. -1.  0.  0.]\n","  [ 0. -1.  0.  0.]\n","  [-1. -1.  0. -1.]\n","  [-1.  0.  0.  0.]\n","  [-1.  0.  0.  0.]]\n","\n"," [[ 0.  0.  0.  0.]\n","  [ 0. -1. -1. -1.]\n","  [ 0.  0. -1. -1.]\n","  [-1.  0. -1. -1.]\n","  [ 0.  0.  0.  0.]]\n","\n"," [[ 0. -1.  0.  0.]\n","  [ 0. -1.  0.  0.]\n","  [-1. -1. -1.  0.]\n","  [-1.  0.  0.  0.]\n","  [-1.  0.  0.  0.]]\n","\n"," [[ 0.  0.  0.  0.]\n","  [ 0.  0. -1.  0.]\n","  [ 0.  0. -1.  0.]\n","  [ 0.  0. -1.  0.]\n","  [ 0.  0.  0.  0.]]]\n"]}]},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**Exercise 4: Reinforcement Learning Application to Management Resources**</FONT>\n","---\n","---"],"metadata":{"id":"AYTTQvEBawzM"}},{"cell_type":"code","source":["# IMPORT COMMON LIBRARIES\n","import numpy as np"],"metadata":{"id":"NZmGs0zIcADE","executionInfo":{"status":"ok","timestamp":1731769175879,"user_tz":300,"elapsed":203,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# DEFINITION OF THE STATES\n","states = ['low','medium','high']"],"metadata":{"id":"0Wi_1M2GcGLm","executionInfo":{"status":"ok","timestamp":1731769177810,"user_tz":300,"elapsed":252,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# DEFINITION OF THE ACTIONS\n","actions = ['Replenish','Do not replenish']"],"metadata":{"id":"nbCuK9fjcOnZ","executionInfo":{"status":"ok","timestamp":1731769238605,"user_tz":300,"elapsed":205,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# REWARD DEFINITION\n","rewards = {\n","  ('low','Replenish'): 50,\n","  ('low','Do not replenish'): -10,\n","  ('medium','Replenish'): 30,\n","  ('medium','Do not replenish'): 0,\n","  ('high','Replenish'): 10,\n","  ('high','Do not replenish'): -20\n","}"],"metadata":{"id":"nblt53-XcZ6j","executionInfo":{"status":"ok","timestamp":1731769181183,"user_tz":300,"elapsed":208,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# Q-LEARNING IMPLEMENTATION\n","Q = {}"],"metadata":{"id":"P8UfqdO-dKc9","executionInfo":{"status":"ok","timestamp":1731769184854,"user_tz":300,"elapsed":190,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# TRAIN PARAMETERS\n","gamma = 0.9 # Discount factor\n","alpha = 0.1 # Learning rate\n","num_episodes = 1000"],"metadata":{"id":"NAnynwgddQpM","executionInfo":{"status":"ok","timestamp":1731769187061,"user_tz":300,"elapsed":197,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# TRAIN LOOP\n","for _ in range(num_episodes):\n","  actual_state = np.random.choice(states)\n","  while True:\n","    action = np.random.choice(actions)\n","    reward = rewards[(actual_state, action)]\n","    if actual_state not in Q:\n","      Q[actual_state] = {}\n","    if action not in Q[actual_state]:\n","      Q[actual_state][action] =0\n","\n","    new_state = np.random.choice(states)\n","    max_new_state = max(Q[new_state].values()) if new_state in Q else 0\n","    Q[actual_state][action] += alpha * (reward + gamma * max_new_state - Q[actual_state][action])\n","    # New State\n","    actual_state = new_state\n","    # Break Point Loop\n","    if reward == 50 or reward == 30 or reward == 10:\n","      break"],"metadata":{"id":"C7vBw8rZdY3k","executionInfo":{"status":"ok","timestamp":1731769251055,"user_tz":300,"elapsed":188,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# Q-VALUES AFTER TRAIN\n","print('Q-Values after training:')\n","print(Q)"],"metadata":{"id":"-SdhtWFrfdrC","executionInfo":{"status":"ok","timestamp":1731769270290,"user_tz":300,"elapsed":206,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"4ca2ddb7-28b9-439f-aff3-42fc9e70b35e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-Values after training:\n","{'low': {'Do not replenish': 243.55056025660724, 'Replenish': 295.51119427657306}, 'medium': {'Do not replenish': 251.72363992408145, 'Replenish': 284.0966048720111}, 'high': {'Replenish': 258.4381776620773, 'Do not replenish': 228.74590917475166}}\n"]}]}]}