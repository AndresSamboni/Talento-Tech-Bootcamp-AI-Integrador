{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["YLbgKiT75vB_","XEqaLIxR7eZz","KX500-VZ9Pss"],"authorship_tag":"ABX9TyMwOsFZLuH9BJeddvcxfl1I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <FONT COLOR=\"red\">***RL ALGORITHMS***</FONT>\n","---\n","---\n","\n","Reinforcement learning (RL) is a branch of machine learning where Agents learn to make optimal decisions through interaction with a around. This set of techniques is essential for situations in which a agent takes actions in an environment to maximize a cumulative reward over time."],"metadata":{"id":"Q5PXxRp85PHH"}},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**Introduction to the main RL algorithms**</FONT>\n","---\n","---\n","\n","Generate a simple game environment where an agent must learn to navigate and collect items. Dene statuses, actions and rewards for the agent"],"metadata":{"id":"YLbgKiT75vB_"}},{"cell_type":"code","execution_count":48,"metadata":{"id":"EepccmUG499y","executionInfo":{"status":"ok","timestamp":1730557104682,"user_tz":300,"elapsed":224,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"outputs":[],"source":["# ENVIRONMENT CLASS\n","class Environment:\n","  # CONSTRUCTOR CLASS\n","  def __init__ (self):\n","    self._state_space = [0,1,2,3]         # Posible states\n","    self._action_space = [0,1]            # Posible actions\n","    self._rewards = {0:-1,1:-1,2:-1,3:10} # Rewards for each state\n","\n","  # GETTERS\n","  @property\n","  def get_state_space(self):\n","    return self._state_space\n","\n","  @property\n","  def get_action_space(self):\n","    return self._action_space\n","\n","  @property\n","  def get_rewards(self):\n","    return self._rewards"]},{"cell_type":"code","source":["# ENVIRONMENT INSTANCE\n","env = Environment()"],"metadata":{"id":"JEffb6Dg6zFr","executionInfo":{"status":"ok","timestamp":1730557104899,"user_tz":300,"elapsed":8,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# TEST IMPLEMENTATION\n","print(f'States: {env.get_state_space}')\n","print(f'Actions: {env.get_action_space}')\n","print(f'Rewards: {env.get_rewards}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5P0W1jXY64Eb","executionInfo":{"status":"ok","timestamp":1730557104899,"user_tz":300,"elapsed":7,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"b46560ca-d813-4e8b-9643-970ea68a2838"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["States: [0, 1, 2, 3]\n","Actions: [0, 1]\n","Rewards: {0: -1, 1: -1, 2: -1, 3: 10}\n"]}]},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**EXERCISE 2**</FONT>\n","---\n","---\n","\n","Implements the Q-Learning algorithm so that an agent learn to navigate and collect objects in the defined environment. Show how the Q-value function is updated."],"metadata":{"id":"XEqaLIxR7eZz"}},{"cell_type":"code","source":["# IMPORT COMMON LIBRARIES\n","import numpy as np"],"metadata":{"id":"6S5bpXwg7q5C","executionInfo":{"status":"ok","timestamp":1730557104900,"user_tz":300,"elapsed":6,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# INITIALIZE THE Q-TABLE\n","Q = np.zeros((len(env.get_state_space), len(env.get_action_space)))"],"metadata":{"id":"gLF5ljr17xbE","executionInfo":{"status":"ok","timestamp":1730557104900,"user_tz":300,"elapsed":6,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# INITIALIZE THE ALGORITHM PARAMETERS\n","alpha = 0.1   # Learning rate\n","gamma = 0.9   # Discount factor"],"metadata":{"id":"EWte59LR8BHV","executionInfo":{"status":"ok","timestamp":1730557104900,"user_tz":300,"elapsed":5,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# TRAIN THE AGENT USING Q-LEARNING\n","for _ in range(1000):\n","  state = np.random.choice(env.get_state_space)     # Random initial state\n","  while state != 3:\n","    action = np.random.choice(env.get_action_space) # Select random action\n","    next_state = state + action\n","    reward = env.get_rewards[next_state]\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","    state = next_state"],"metadata":{"id":"KW_qJ4-38PB0","executionInfo":{"status":"ok","timestamp":1730557105130,"user_tz":300,"elapsed":234,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# TEST IMPLEMENTATION\n","print(f'Q-Value function learned:\\n{Q}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KeNoOnjh9CQZ","executionInfo":{"status":"ok","timestamp":1730557105130,"user_tz":300,"elapsed":13,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"ae27e582-76e9-49ed-bc82-b0a9368c481b"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-Value function learned:\n","[[ 4.58  6.2 ]\n"," [ 6.2   8.  ]\n"," [ 8.   10.  ]\n"," [ 0.    0.  ]]\n"]}]},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**EXERCISE 3**</FONT>\n","---\n","---\n","\n","Implement the Sarsa algorithm to\n","compare it with Q-Learning in the same\n","around. Shows how to update the\n","Q-value function and compare the results"],"metadata":{"id":"KX500-VZ9Pss"}},{"cell_type":"code","source":["# INITIALIZE Q-TABLE\n","Q = np.zeros((len(env.get_state_space), len(env.get_action_space)))"],"metadata":{"id":"Knbe_VTj9YHT","executionInfo":{"status":"ok","timestamp":1730557105130,"user_tz":300,"elapsed":11,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# TRAIN THE SARSA AGENT\n","for _ in range(1000):\n","  state = np.random.choice(env.get_state_space)          # Random initial state\n","  action = np.random.choice(env.get_action_space)        # Select random action\n","  while state != 3:\n","    next_state = state + action\n","    next_action = np.random.choice(env.get_action_space) # Select random next action\n","    reward = env.get_rewards[next_state]\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n","    state = next_state\n","    action = next_action"],"metadata":{"id":"0opAe1B_9uto","executionInfo":{"status":"ok","timestamp":1730557105131,"user_tz":300,"elapsed":11,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# TEST IMPLEMENTATION\n","print(f'Q-Value function learned with Sarsa:\\n{Q}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DD_bbmt4-d7V","executionInfo":{"status":"ok","timestamp":1730557105132,"user_tz":300,"elapsed":11,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}},"outputId":"2631926a-95ff-4d4d-a141-aa58d4162d36"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-Value function learned with Sarsa:\n","[[ 0.54618649  3.74783604]\n"," [ 3.85663046  7.22931891]\n"," [ 6.90560404 10.        ]\n"," [ 0.          0.        ]]\n"]}]},{"cell_type":"markdown","source":["## <FONT COLOR=\"orange\">**EXERCISE 4**</FONT>\n","---\n","---\n","\n","Implements the optimization technique based on gradients to learn a policy in the same environment. Shows how policy parameters are updated using gradient ascending."],"metadata":{"id":"Hoz_gvSD_r0N"}},{"cell_type":"code","source":["# INITIALIZE POLICY WITH UNIFORM PROBABILITIES\n","policy = np.ones(\n","  (\n","    len(env.get_state_space),\n","    len(env.get_action_space)\n","  )\n",")/len(env.get_action_space)"],"metadata":{"id":"XxmxW91SAOst","executionInfo":{"status":"ok","timestamp":1730558647916,"user_tz":300,"elapsed":248,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["# AVERAGE REWARD\n","def average_reward(Q):\n","  return np.mean(\n","    [\n","      Q[\n","        state,\n","        np.argmax(policy[state])\n","      ] for state in env.get_state_space\n","    ]\n","  )"],"metadata":{"id":"OdzuAq26AecQ","executionInfo":{"status":"ok","timestamp":1730557105323,"user_tz":300,"elapsed":200,"user":{"displayName":"EDWIN ANDRES SAMBONI ORTIZ","userId":"14774678352093959572"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["# TRAIN AGENT WITH MONTECARLO'S GRADIENT\n","for _ in range(1000):\n","  state = np.random.choice(env.get_state_space)           # Initial State\n","  while state != 3:                                       # Objetive State\n","    # SELECT AN ACTION RELY ON THE ACTUAL POLICY\n","    action = np.random.choice(\n","      env.get_action_space,\n","      p=policy[state]\n","    )\n","    next_state = state + action                           # Identify next state\n","    reward = env.get_rewards[next_state]                  # Obtain reward\n","    gradient = np.zeros_like(policy[state])\n","    gradient[action] = 1\n","    alpha = 0.01                                          # Learning rate\n","    policy[state] += alpha * gradient * (reward - average_reward(Q))\n","    policy[state] = np.clip(policy[state], 0, 1)          # Clip policy values\n","    policy[state] = policy[state] / np.sum(policy[state]) # Policy normalization\n","    state = next_state"],"metadata":{"id":"r9FJAB6sAwEb"},"execution_count":null,"outputs":[]}]}